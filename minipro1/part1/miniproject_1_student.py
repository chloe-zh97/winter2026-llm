## Mini Project 1 - Part 1: Getting Familiar with Word Embeddings.
# This assignment introduces students to text similarity measures using cosine similarity and sentence embeddings. 
# Students will implement and compare different methods for computing and analyzing text similarity using GloVe, Sentence Transformers, and OpenAI Embeddings.

#Learning Objectives
#By the end of this assignment, students will:
#Understand how cosine similarity is used to measure text similarity.
#Learn to encode sentences using GloVe embeddings, Sentence Transformers, and OpenAI embeddings.
#Compare the performance of different embedding techniques (small vs large models).
#Create a Web interface for your model
#Analyze intuitive examples like "Chocolate Milk" vs "Milk Chocolate" to understand embedding behavior

# Context: In this part, you are going to play around with some commonly used pretrained text embeddings for text search. For example, GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Pretrained on 
# 2 billion tweets with vocabulary size of 1.2 million. Download from [Stanford NLP](http://nlp.stanford.edu/data/glove.twitter.27B.zip). 
# Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. *GloVe: Global Vectors for Word Representation*.

### Import necessary libraries: here you will use streamlit library to run a text search demo, please make sure to install it.
import streamlit as st
import numpy as np
import numpy.linalg as la
import pickle
import os
import gdown
from sentence_transformers import SentenceTransformer
import matplotlib.pyplot as plt
import math
from openai import OpenAI  # Added for OpenAI embeddings
import time
import json
import pandas as pd


### Some predefined utility functions for you to load the text embeddings

# Function to Load Glove Embeddings
def load_glove_embeddings(glove_path="Data/embeddings.pkl"):
    with open(glove_path, "rb") as f:
        embeddings_dict = pickle.load(f, encoding="latin1")

    return embeddings_dict


def get_model_id_gdrive(model_type):
    if model_type == "25d":
        word_index_id = "13qMXs3-oB9C6kfSRMwbAtzda9xuAUtt8"
        embeddings_id = "1-RXcfBvWyE-Av3ZHLcyJVsps0RYRRr_2"
    elif model_type == "50d":
        embeddings_id = "1DBaVpJsitQ1qxtUvV1Kz7ThDc3az16kZ"
        word_index_id = "1rB4ksHyHZ9skes-fJHMa2Z8J1Qa7awQ9"
    elif model_type == "100d":
        word_index_id = "1-oWV0LqG3fmrozRZ7WB1jzeTJHRUI3mq"
        embeddings_id = "1SRHfX130_6Znz7zbdfqboKosz-PfNvNp"
        
    return word_index_id, embeddings_id


def download_glove_embeddings_gdrive(model_type):
    # Get glove embeddings from google drive
    word_index_id, embeddings_id = get_model_id_gdrive(model_type)

    # Use gdown to get files from google drive
    embeddings_temp = "embeddings_" + str(model_type) + "_temp.npy"
    word_index_temp = "word_index_dict_" + str(model_type) + "_temp.pkl"

    # Download word_index pickle file
    print("Downloading word index dictionary....\n")
    gdown.download(id=word_index_id, output=word_index_temp, quiet=False)

    # Download embeddings numpy file
    print("Donwloading embedings...\n\n")
    gdown.download(id=embeddings_id, output=embeddings_temp, quiet=False)


# @st.cache_data()
def load_glove_embeddings_gdrive(model_type):
    word_index_temp = "word_index_dict_" + str(model_type) + "_temp.pkl"
    embeddings_temp = "embeddings_" + str(model_type) + "_temp.npy"

    # Load word index dictionary
    word_index_dict = pickle.load(open(word_index_temp, "rb"), encoding="latin")

    # Load embeddings numpy
    embeddings = np.load(embeddings_temp)

    return word_index_dict, embeddings


@st.cache_resource()
def load_sentence_transformer_model(model_name):
    sentenceTransformer = SentenceTransformer(model_name)
    return sentenceTransformer


@st.cache_resource()
def load_openai_client():
    """
    Load OpenAI client (cached to avoid multiple initializations)
    Make sure to set OPENAI_API_KEY environment variable
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        st.warning("OpenAI API key not found. Please set OPENAI_API_KEY environment variable.")
        return None
    return OpenAI(api_key=api_key)


def get_openai_embeddings(sentence, model_name="text-embedding-3-small"):
    """
    Get OpenAI embeddings for a sentence
    
    Available models:
    - text-embedding-3-small: 1536 dimensions, faster and cheaper
    - text-embedding-3-large: 3072 dimensions, higher performance
    
    Args:
        sentence: Input text to embed
        model_name: OpenAI embedding model to use
    
    Returns:
        numpy array of embeddings
    """
    client = load_openai_client()
    if client is None:
        # Return zero vector if API key not available
        if model_name == "text-embedding-3-small":
            return np.zeros(1536)
        else:  # text-embedding-3-large
            return np.zeros(3072)
    
    try:
        response = client.embeddings.create(
            input=sentence,
            model=model_name
        )
        return np.array(response.data[0].embedding)
    except Exception as e:
        st.error(f"Error getting OpenAI embeddings: {e}")
        if model_name == "text-embedding-3-small":
            return np.zeros(1536)
        else:
            return np.zeros(3072)


def get_sentence_transformer_embeddings(sentence, model_name="all-MiniLM-L6-v2"):
    """
    Get sentence transformer embeddings for a sentence
    """
    # 384 dimensional embedding
    # Default model: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  

    sentenceTransformer = load_sentence_transformer_model(model_name)

    try:
        return sentenceTransformer.encode(sentence)
    except:
        if model_name == "all-MiniLM-L6-v2":
            return np.zeros(384)
        else:
            return np.zeros(512)


def get_glove_embeddings(word, word_index_dict, embeddings, model_type):
    """
    Get glove embedding for a single word
    """
    if word.lower() in word_index_dict:
        return embeddings[word_index_dict[word.lower()]]
    else:
        return np.zeros(int(model_type.split("d")[0]))


def get_category_embeddings(embeddings_metadata):
    """
    Get embeddings for each category
    1. Split categories into words
    2. Get embeddings for each word
    """
    model_name = embeddings_metadata["model_name"]
    embedding_model = embeddings_metadata["embedding_model"]
    
    cache_key = "cat_embed_" + embedding_model + "_" + model_name
    st.session_state[cache_key] = {}
    
    for category in st.session_state.categories.split(" "):
        if embedding_model == "openai":
            if category not in st.session_state[cache_key]:
                st.session_state[cache_key][category] = get_openai_embeddings(category, model_name=model_name)
        elif model_name:
            if category not in st.session_state[cache_key]:
                st.session_state[cache_key][category] = get_sentence_transformer_embeddings(category, model_name=model_name)
        else:
            if category not in st.session_state[cache_key]:
                st.session_state[cache_key][category] = get_sentence_transformer_embeddings(category)


def update_category_embeddings(embeddings_metadata):
    """
    Update embeddings for each category
    """
    get_category_embeddings(embeddings_metadata)




### Plotting utility functions
    
def plot_piechart(sorted_cosine_scores_items):
    sorted_cosine_scores = np.array([
            sorted_cosine_scores_items[index][1]
            for index in range(len(sorted_cosine_scores_items))
        ]
    )
    categories = st.session_state.categories.split(" ")
    categories_sorted = [
        categories[sorted_cosine_scores_items[index][0]]
        for index in range(len(sorted_cosine_scores_items))
    ]
    fig, ax = plt.subplots()
    ax.pie(sorted_cosine_scores, labels=categories_sorted, autopct="%1.1f%%")
    st.pyplot(fig)  # Figure


def plot_piechart_helper(sorted_cosine_scores_items):
    sorted_cosine_scores = np.array(
        [
            sorted_cosine_scores_items[index][1]
            for index in range(len(sorted_cosine_scores_items))
        ]
    )
    categories = st.session_state.categories.split(" ")
    categories_sorted = [
        categories[sorted_cosine_scores_items[index][0]]
        for index in range(len(sorted_cosine_scores_items))
    ]
    fig, ax = plt.subplots(figsize=(3, 3))
    my_explode = np.zeros(len(categories_sorted))
    my_explode[0] = 0.2
    if len(categories_sorted) == 3:
        my_explode[1] = 0.1  # explode this by 0.2
    elif len(categories_sorted) > 3:
        my_explode[2] = 0.05
    ax.pie(
        sorted_cosine_scores,
        labels=categories_sorted,
        autopct="%1.1f%%",
        explode=my_explode,
    )

    return fig


def plot_piecharts(sorted_cosine_scores_models):
    scores_list = []
    categories = st.session_state.categories.split(" ")
    index = 0
    for model in sorted_cosine_scores_models:
        scores_list.append(sorted_cosine_scores_models[model])
        # scores_list[index] = np.array([scores_list[index][ind2][1] for ind2 in range(len(scores_list[index]))])
        index += 1

    if len(sorted_cosine_scores_models) == 2:
        fig, (ax1, ax2) = plt.subplots(2)

        categories_sorted = [
            categories[scores_list[0][index][0]] for index in range(len(scores_list[0]))
        ]
        sorted_scores = np.array(
            [scores_list[0][index][1] for index in range(len(scores_list[0]))]
        )
        ax1.pie(sorted_scores, labels=categories_sorted, autopct="%1.1f%%")

        categories_sorted = [
            categories[scores_list[1][index][0]] for index in range(len(scores_list[1]))
        ]
        sorted_scores = np.array(
            [scores_list[1][index][1] for index in range(len(scores_list[1]))]
        )
        ax2.pie(sorted_scores, labels=categories_sorted, autopct="%1.1f%%")

    st.pyplot(fig)


def plot_alatirchart(sorted_cosine_scores_models):
    models = list(sorted_cosine_scores_models.keys())
    tabs = st.tabs(models)
    figs = {}
    for model in models:
        figs[model] = plot_piechart_helper(sorted_cosine_scores_models[model])

    for index in range(len(tabs)):
        with tabs[index]:
            st.pyplot(figs[models[index]])


### Your Part To Complete: Follow the instructions in each function below to complete the similarity calculation between text embeddings

# Task I: Compute Cosine Similarity
def cosine_similarity(x, y):
    """
    Exponentiated cosine similarity
    1. Compute cosine similarity
    2. Exponentiate cosine similarity
    3. Return exponentiated cosine similarity
    (20 pts)
    """
    ##################################
    ### TODO: Add code here (10 pts) ###
    ##################################
    dot_product = np.dot(x, y)

    norm_x = np.linalg.norm(x)
    norm_y = np.linalg.norm(y)

    if norm_x == 0 or norm_y == 0:
        return 0.0
    cos_sim = dot_product / (norm_x * norm_y)
    return np.exp(cos_sim)  
    
# Task II: Average Glove Embedding Calculation
def averaged_glove_embeddings_gdrive(sentence, word_index_dict, embeddings, model_type="50d"):
    """
    Get averaged glove embeddings for a sentence
    1. Split sentence into words
    2. Get embeddings for each word
    3. Add embeddings for each word
    4. Divide by number of words
    5. Return averaged embeddings
    (30 pts)
    """
    embedding_dim = int(model_type.split("d")[0])
    embedding = np.zeros(embedding_dim)
    
    ##################################
    ##### TODO: Add code here (20 pts) #####
    ##################################
    # 1. split sentence into words
    words = sentence.lower().replace(",", "").replace(".", "").split()

    # 2. get and add embedding for each word
    word_count = 0
    for word in words:
        word_embedding = get_glove_embeddings(word, word_index_dict, embeddings, model_type)
        # Add to accumulated embedding
        embedding += word_embedding
        word_count += 1
    
    if word_count > 0:
        embedding /= word_count

    return embedding  


# Task III: Sort the cosine similarity
def get_sorted_cosine_similarity(embeddings_metadata, input_text=None):
    """
    Get sorted cosine similarity between input sentence and categories
    
    Steps:
    1. Get embeddings for input sentence
    2. Get embeddings for categories (if not found, update category embeddings)
    3. Compute cosine similarity between input sentence and categories
    4. Sort cosine similarity
    5. Return sorted cosine similarity
    
    (50 pts)

    NOTE: This function must handle THREE different embedding types:
    - GloVe Embeddings (word averaging)
    - OpenAI Embeddings (API-based, small & large models)
    - Sentence Transformer Embeddings (context-aware)
    
    IMPORTANT: 
    - Each embedding type requires different methods to get embeddings
    - Use caching (st.session_state) to avoid redundant computations
    - Return results sorted by similarity (highest first)
    
    Args:
        embeddings_metadata: Dictionary containing:
            - "embedding_model": Type of model ("glove", "openai", or "transformers")
            - For GloVe: "word_index_dict", "embeddings", "model_type"
            - For OpenAI/Transformers: "model_name"
    
    Returns:
        List of tuples: [(category_index, similarity_score), ...]
        Sorted in descending order by similarity_score
    
    Available helper functions:
        - averaged_glove_embeddings_gdrive(sentence, word_index_dict, embeddings, model_type)
        - get_openai_embeddings(sentence, model_name)
        - get_sentence_transformer_embeddings(sentence, model_name)
        - get_category_embeddings(embeddings_metadata)
        - cosine_similarity(x, y)
    """
    if input_text is None:
        input_text = st.session_state.text_search
    
    categories = st.session_state.categories.split(" ")
    cosine_sim = {}
    input_sentence = st.session_state.text_search
    
    if embeddings_metadata["embedding_model"] == "glove":
        # Extract GloVe-specific parameters
        word_index_dict = embeddings_metadata["word_index_dict"]
        embeddings = embeddings_metadata["embeddings"]
        model_type = embeddings_metadata["model_type"]

        ##########################################
        ## TODO: Implement GloVe similarity calculation (15 pts)
        ##########################################
        # 1. get input sentence embedding
        input_embedding = averaged_glove_embeddings_gdrive(
            input_sentence, 
            word_index_dict, 
            embeddings, 
            model_type)
        
        # 2. category embeddings, using cache
        for idx, category in enumerate(categories):
            # Get embedding for this category
            category_embedding = averaged_glove_embeddings_gdrive(
                category, 
                word_index_dict, 
                embeddings, 
                model_type
            )
            
            # 3. compute cosine similarity
            similarity = cosine_similarity(input_embedding, category_embedding)
            cosine_sim[idx] = similarity
        
    elif embeddings_metadata["embedding_model"] == "openai":
        # Extract OpenAI-specific parameters
        model_name = embeddings_metadata["model_name"]
        cache_key = "cat_embed_openai_" + model_name
        
        ##########################################
        ## TODO: Implement OpenAI similarity calculation (15 pts)
        ##########################################
        # if cache_key not in st.session_state:
        #     # Make sure embeddings_metadata has the correct structure for get_category_embeddings
        #     category_metadata = {
        #         "embedding_model": "openai",
        #         "model_name": model_name
        #     }
        #     get_category_embeddings(category_metadata)
        if cache_key not in st.session_state:
            st.session_state[cache_key] = {}

        # get input embedding
        input_embedding = get_openai_embeddings(input_sentence, model_name)

        # calculate cosine
        for idx, category in enumerate(categories):
            if category not in st.session_state[cache_key]:  # ‚Üê ADD THIS CHECK
                st.session_state[cache_key][category] = get_openai_embeddings(
                category, model_name=model_name)
            category_embedding = st.session_state[cache_key][category]
            similarity = cosine_similarity(input_embedding, category_embedding)
            cosine_sim[idx] = similarity
        

    else:  # Sentence transformers
        # Extract Sentence Transformer-specific parameters
        model_name = embeddings_metadata["model_name"]
        cache_key = "cat_embed_transformers_" + (model_name if model_name else "default")
        
        ##########################################
        ## TODO: Implement Sentence Transformer similarity calculation (15 pts)
        ##########################################
        # if cache_key not in st.session_state:
        #     # Make sure embeddings_metadata has the correct structure for get_category_embeddings
        #     category_metadata = {
        #         "embedding_model": "transformers",
        #         "model_name": model_name
        #     }
        #     get_category_embeddings(category_metadata)
        if cache_key not in st.session_state:
            st.session_state[cache_key] = {}
        
        # input embedding
        input_embedding = get_sentence_transformer_embeddings(
            input_sentence, model_name
        )

        # calculate cosine
        for idx, category in enumerate(categories):
            if category not in st.session_state[cache_key]:  # ‚Üê ADD THIS CHECK
                st.session_state[cache_key][category] = get_sentence_transformer_embeddings(
                category, model_name=model_name
            )
            category_embedding = st.session_state[cache_key][category]
            similarity = cosine_similarity(input_embedding, category_embedding)
            cosine_sim[idx] = similarity
        
    ##########################################
    ## TODO: Sort and return results (5 pts)
    ##########################################
    res = sorted(cosine_sim.items(), key=lambda x: x[1], reverse=True)
    return res  

### Below is the main function, creating the app demo for text search engine using the text embeddings.

if __name__ == "__main__":
    ### Text Search ###
    ### There will be Bonus marks of 10% for the teams that submit a URL for your deployed web app. 
    ### Bonus: You can also submit a publicly accessible link to the deployed web app.

    st.sidebar.title("Embedding Models")
    st.sidebar.markdown(
        """
    Compare different embedding models:
    - **GloVe**: Pretrained on 2 billion tweets
    - **Sentence Transformers**: Fine-tuned on sentence similarity tasks
    - **OpenAI Small**: text-embedding-3-small (1536d)
    - **OpenAI Large**: text-embedding-3-large (3072d)
    
    [GloVe Paper](http://nlp.stanford.edu/data/glove.twitter.27B.zip) | 
    [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)
    """
    )

    model_type = st.sidebar.selectbox("Choose the model", ("25d", "50d", "100d"), index=1)

    # add new test cases
    st.sidebar.markdown("---")
    st.sidebar.subheader("Test Cases for Comparison")
    test_cases = {
        "Colors Focus": "Roses are red, trucks are blue, and Seattle is grey right now",
        "Flowers Focus": "I love tulips, roses, and daisies in my garden",
        "Weather Focus": "It's raining in Seattle, cloudy in Portland, and sunny in LA",
        "Cars Focus": "Tesla, Ford, and Toyota are popular car brands",
        "Food Focus": "Pizza, pasta, and sushi are my favorite foods",
        "Word Order 1": "Chocolate Milk",
        "Word Order 2": "Milk Chocolate",
        "Ambiguous": "Apple released a new product today",
        "Multi-Category": "I ate pizza while watching cars race in the rain",
        "Complex": "The weather is perfect for driving my new car to the flower market"
    }

    selected_test = st.sidebar.selectbox(
        "Select a test case:",
        ["Custom Input"] + list(test_cases.keys())
    )

    # Set default text based on selection
    if selected_test == "Custom Input":
        default_text = "Roses are red, trucks are blue, and Seattle is grey right now"
    else:
        default_text = test_cases[selected_test]
        st.sidebar.info(f"Testing: {selected_test}")

    # batch test mode
    st.sidebar.markdown("---")
    st.sidebar.subheader("Batch Test Mode")
    batch_test = st.sidebar.checkbox("Enable Batch Testing (runs all test cases)")


    st.title("Search Based Retrieval Demo")
    st.subheader(
        "Pass in space separated categories you want this search demo to be about."
    )
    # st.selectbox(label="Pick the categories you want this search demo to be about...",
    # options=("Flowers Colors Cars Weather Food", "Chocolate Milk", "Anger Joy Sad Frustration Worry Happiness", "Positive Negative"),
    # key="categories"
    # )
    st.text_input(
        label="Categories", key="categories", value="Flowers Colors Cars Weather Food"
    )
    print(st.session_state["categories"])
    print(type(st.session_state["categories"]))
    # print("Categories = ", categories)
    # st.session_state.categories = categories

    st.subheader("Pass in an input word or even a sentence")
    text_search = st.text_input(
        label="Input your sentence",
        key="text_search",
        value=default_text,
    )
    # st.session_state.text_search = text_search

    # Download glove embeddings if it doesn't exist
    embeddings_path = "embeddings_" + str(model_type) + "_temp.npy"
    word_index_dict_path = "word_index_dict_" + str(model_type) + "_temp.pkl"
    if not os.path.isfile(embeddings_path) or not os.path.isfile(word_index_dict_path):
        print("Model type = ", model_type)
        glove_path = "Data/glove_" + str(model_type) + ".pkl"
        print("glove_path = ", glove_path)

        # Download embeddings from google drive
        with st.spinner("Downloading glove embeddings..."):
            download_glove_embeddings_gdrive(model_type)


    # Load glove embeddings
    word_index_dict, embeddings = load_glove_embeddings_gdrive(model_type)
    
    # batch test execution
    if batch_test and st.sidebar.button("üöÄ Run All Tests"):
        st.markdown("---")
        st.header("Batch Test Results")

        all_test_results = []
        categories = st.session_state.categories.split(" ")

        progress_bar = st.progress(0)
        status_text = st.empty()

        for idx, (test_name, test_input) in enumerate(test_cases.items()):
            status_text.text(f"Testing: {test_name}...")
            progress_bar.progress((idx + 1) / len(test_cases))
            
            # Temporarily set text_search
            original_text = st.session_state.text_search
            #st.session_state.text_search = test_input
            
            test_result = {
                "Test Case": test_name,
                "Input": test_input
            }

            # Test each model
            models_config = {
                "GloVe": {"embedding_model": "glove", "word_index_dict": word_index_dict, 
                         "embeddings": embeddings, "model_type": model_type},
                "Transformer": {"embedding_model": "transformers", "model_name": "all-MiniLM-L6-v2"},
                "OpenAI Small": {"embedding_model": "openai", "model_name": "text-embedding-3-small"},
                "OpenAI Large": {"embedding_model": "openai", "model_name": "text-embedding-3-large"}
            }

            for model_name, metadata in models_config.items():
                start = time.time()
                scores = get_sorted_cosine_similarity(metadata, input_text=test_input)
                elapsed = time.time() - start
                
                top_cat = categories[scores[0][0]]
                top_score = scores[0][1]
                
                test_result[f"{model_name} Prediction"] = top_cat
                test_result[f"{model_name} Score"] = f"{top_score:.4f}"
                test_result[f"{model_name} Time (ms)"] = f"{elapsed*1000:.2f}"

            all_test_results.append(test_result)
            #st.session_state.text_search = original_text

        status_text.text("‚úÖ All tests completed!")
        progress_bar.empty()

        # Display results
        st.markdown("### üìä Complete Test Results")
        batch_df = pd.DataFrame(all_test_results)
        st.dataframe(batch_df, use_container_width=True)

        # Download option
        batch_csv = batch_df.to_csv(index=False)
        st.download_button(
            label="üì• Download Batch Results (CSV)",
            data=batch_csv,
            file_name=f"batch_test_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )

        # Analysis summary
        st.markdown("### üìà Model Agreement Analysis")

        # Check how often models agree
        agreement_data = []
        for result in all_test_results:
            predictions = [
                result["GloVe Prediction"],
                result["Transformer Prediction"],
                result["OpenAI Small Prediction"],
                result["OpenAI Large Prediction"]
            ]
            agreement = len(set(predictions)) == 1  # All same
            agreement_data.append({
                "Test Case": result["Test Case"],
                "All Agree": "‚úÖ Yes" if agreement else "‚ùå No",
                "Unique Predictions": ", ".join(set(predictions))
            })

        agreement_df = pd.DataFrame(agreement_data)
        st.dataframe(agreement_df, use_container_width=True)
        
        st.markdown("---")

    # Find closest word to an input word
    if st.session_state.text_search and not batch_test:
        results_dict = {}
        
        # Glove embeddings
        print("Glove Embedding")
        embeddings_metadata = {
            "embedding_model": "glove",
            "word_index_dict": word_index_dict,
            "embeddings": embeddings,
            "model_type": model_type,
        }
        with st.spinner("Obtaining Cosine similarity for Glove..."):
            sorted_cosine_sim_glove = get_sorted_cosine_similarity(embeddings_metadata)
            results_dict["glove_" + str(model_type)] = sorted_cosine_sim_glove

        # Sentence transformer embeddings
        print("Sentence Transformer Embedding")
        embeddings_metadata = {
            "embedding_model": "transformers", 
            "model_name": "all-MiniLM-L6-v2"
        }
        with st.spinner("Obtaining Cosine similarity for 384d sentence transformer..."):
            sorted_cosine_sim_transformer = get_sorted_cosine_similarity(embeddings_metadata)
            results_dict["sentence_transformer_384"] = sorted_cosine_sim_transformer

        # OpenAI Small embeddings
        print("OpenAI Small Embedding")
        embeddings_metadata = {
            "embedding_model": "openai", 
            "model_name": "text-embedding-3-small"
        }
        with st.spinner("Obtaining Cosine similarity for OpenAI Small (1536d)..."):
            sorted_cosine_sim_openai_small = get_sorted_cosine_similarity(embeddings_metadata)
            results_dict["openai_small_1536"] = sorted_cosine_sim_openai_small

        # OpenAI Large embeddings
        print("OpenAI Large Embedding")
        embeddings_metadata = {
            "embedding_model": "openai", 
            "model_name": "text-embedding-3-large"
        }
        with st.spinner("Obtaining Cosine similarity for OpenAI Large (3072d)..."):
            sorted_cosine_sim_openai_large = get_sorted_cosine_similarity(embeddings_metadata)
            results_dict["openai_large_3072"] = sorted_cosine_sim_openai_large

        # Results and Plot Pie Chart for all models
        print("Categories are: ", st.session_state.categories)
        st.subheader(
            "Closest category between: "
            + st.session_state.categories
            + " as per different Embeddings"
        )

        # Display results in tabs
        plot_alatirchart(results_dict)
        
        # Add comparison table
        st.markdown("---")
        st.subheader("Detailed Comparison")
        
        categories = st.session_state.categories.split(" ")
        comparison_data = []
        
        for model_name, scores in results_dict.items():
            top_category_idx = scores[0][0]
            top_category = categories[top_category_idx]
            top_score = scores[0][1]
            comparison_data.append({
                "Model": model_name,
                "Top Category": top_category,
                "Confidence Score": f"{top_score:.4f}"
            })
        
        import pandas as pd
        df = pd.DataFrame(comparison_data)
        st.table(df)

        st.write("")
        st.write(
            "Demo developed by [Your Name](https://www.linkedin.com/in/your_id/ - Optional)"
        )
        

        
