{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project Part-3: Building a Multi-Agent Chatbot (50 points)\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this assignment is to build a chatbot that utilizes multiple agents, each with a specific role, and a controller agent that manages these sub-agents. The chatbot should be able to handle user queries, check for obnoxious content, and retrieve relevant documents to assist in generating responses.\n",
    "\n",
    "## Action Items\n",
    "\n",
    "1. **Setup the Environment**: Install necessary libraries such as `openai`, `pinecone`, and any other libraries you might need. Obtain necessary API keys for OpenAI and Pinecone.\n",
    "\n",
    "2. **Implement the Obnoxious Agent**: This agent checks if a user's query is obnoxious. If it is, the agent responds with \"Yes\", otherwise \"No\". Implement this agent using the `Obnoxious_Agent` class as a guide.  \n",
    "  *Restriction on Obnoxious agent: Cannot use Langchain API for this agent.*\n",
    "\n",
    "3. **Implement Relelevant Documents Agent**: This agent retrieves relevant documents. Implement this agent using the `Relevant_Documents_Agent` class as a guide. Also responsible for checking if the retrieved documents are relevant to the user's query.\n",
    "\n",
    "    *Restriction on Relevant agent: Cannot use Langchain API for this agent.*\n",
    "\n",
    "4. **Implement the Pinecone Query Agent**: This agent checks if a user's query is relevant to a specific topic (e.g., a book on Machine Learning) and retrieves relevant documents. Implement this agent using the `Query_Agent` class as a guide.\n",
    "\n",
    "5. **Implement the Answering Agent**: This agent generates a response to the user's query using the relevant documents retrieved by the Pinecone Query Agent. Implement this agent using the `Answering_Agent` class as a guide.\n",
    "\n",
    "6. **Implement the Head Agent**: This is the controller agent that manages the other agents. It determines which agent to use for each query and uses that agent to get a response. Implement this agent using the `Head_Agent` class as a guide.\n",
    "\n",
    "7. **Streamlit App**: Integrate this chatbot into the Streamlit app from Mini-project part-2.\n",
    "\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "1. Python code files for each agent and the controller agent.\n",
    "2. A PDF report that contains a design diagram of your approach along with some screenshots of Streamlit demoing 3-4 test cases\n",
    "\n",
    "\n",
    "## Evaluation Criteria\n",
    "1. Completion: Are all components implemented in a reasonable way? (25 points)\n",
    "2. Documentation: Is the process well-documented, with a diagram and descriptions of challenges and solutions? (20 points)\n",
    "3. Creativity: How creatively has the problem been solved? (5 points)\n",
    "\n",
    "## Notes:\n",
    "- There are no specific constraints on the implementation methods for the agents. However, it is crucial that the agents can interact with each other and the controller agent effectively.\n",
    "- You have the liberty to modify the provided agent classes to fit your implementation strategy.\n",
    "- You can utilize any libraries or APIs to construct the chatbot. However, the use of the Langchain API is prohibited for the Obnoxious and Relevant Documents agents. The Langchain API can be used for the Pinecone Query and Answering agents.\n",
    "- Please use `gpt-4.1-nano` for all agents. \n",
    "- Below we provide some starter code, but feel free to modify it if you have an alternate design in mind\n",
    "\n",
    "## Resources\n",
    "\n",
    "1. [OpenAI API Documentation](https://platform.openai.com/docs/overview)\n",
    "2. [Pinecone Documentation](https://docs.pinecone.io/)\n",
    "3. [Langchain Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "4. [Interesting paper utilizing agents](https://arxiv.org/pdf/2303.17580.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "pcsk_21HrNV_Lw3HbDR4hWjJm6mkiMbfbR8UYdyMGV3jHMMU2GiH9TGj4hPZjpKSrLAChenLVdT\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "load_dotenv()\n",
    "open_ai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "print(open_ai_api_key)\n",
    "print(pinecone_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obnoxious Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python\n",
    "class Obnoxious_Agent:\n",
    "    def __init__(self, client) -> None:\n",
    "        # TODO: Initialize the client and prompt for the Obnoxious_Agent\n",
    "        self.client = client\n",
    "\n",
    "        # Default strict prompt\n",
    "        self.system_prompt = (\n",
    "            \"You are a content moderation agent. \"\n",
    "            \"Your task is to determine whether a user's query is obnoxious, \"\n",
    "            \"offensive, toxic, abusive, hateful, or inappropriate.\\n\\n\"\n",
    "            \"If the query is obnoxious, respond ONLY with: Yes\\n\"\n",
    "            \"If the query is not obnoxious, respond ONLY with: No\\n\"\n",
    "            \"Do not include any explanation.\"\n",
    "        )\n",
    "\n",
    "    def set_prompt(self, prompt):\n",
    "        # TODO: Set the prompt for the Obnoxious_Agent\n",
    "        self.system_prompt = prompt\n",
    "\n",
    "    def extract_action(self, response) -> bool:\n",
    "        # TODO: Extract the action from the response\n",
    "        answer = response.strip().lower()\n",
    "        if \"yes\" in answer: \n",
    "            return True\n",
    "        elif \"no\" in answer:\n",
    "            return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def check_query(self, query):\n",
    "        # TODO: Check if the query is obnoxious or not\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            temperature=0  # deterministic output\n",
    "        )\n",
    "        response_text = completion.choices[0].message.content\n",
    "        return self.extract_action(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obnoxious Agent Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "open_ai_client = OpenAI(api_key=open_ai_api_key)\n",
    "agent = Obnoxious_Agent(open_ai_client)\n",
    "print(agent.check_query(\"You are stupid!\"))   # True\n",
    "print(agent.check_query(\"What is machine learning?\"))  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Rewriter Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context_Rewriter_Agent:\n",
    "    def __init__(self, openai_client):\n",
    "        # TODO: Initialize the Context_Rewriter agent\n",
    "        self.client = openai_client\n",
    "        self.system_prompt = (\n",
    "            \"You are a query rewriting agent.\\n\"\n",
    "            \"Your task is to rewrite the user's latest question into a clear, \"\n",
    "            \"fully self-contained question.\\n\\n\"\n",
    "            \"Rules:\\n\"\n",
    "            \"- Resolve pronouns and ambiguous references using the conversation history.\\n\"\n",
    "            \"- Keep the original meaning.\\n\"\n",
    "            \"- Do NOT answer the question.\\n\"\n",
    "            \"- Only return the rewritten question.\\n\"\n",
    "        )\n",
    "\n",
    "    def rephrase(self, user_history, latest_query):\n",
    "        # TODO: Resolve ambiguities in the final prompt for multiturn situations\n",
    "        # Convert history into readable text block\n",
    "        history_text = \"\"\n",
    "        for i, msg in enumerate(user_history):\n",
    "            history_text += f\"Turn {i+1}: {msg}\\n\"\n",
    "        \n",
    "        user_prompt = (\n",
    "            f\"Conversation History:\\n{history_text}\\n\"\n",
    "            f\"Latest User Question:\\n{latest_query}\\n\\n\"\n",
    "            \"Rewrite the latest question into a standalone, unambiguous question.\"\n",
    "        )\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        rewritten_query = completion.choices[0].message.content.strip()\n",
    "        return rewritten_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Rewriter Agent Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does supervised learning work?\n"
     ]
    }
   ],
   "source": [
    "user_history = [\n",
    "    \"Explain supervised learning.\",\n",
    "    \"What are the common algorithms?\"\n",
    "]\n",
    "\n",
    "latest = \"How does it work?\"\n",
    "\n",
    "agent = Context_Rewriter_Agent(open_ai_client)\n",
    "print(agent.rephrase(user_history, latest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query_Agent:\n",
    "    def __init__(self, pinecone_index, openai_client, embeddings) -> None:\n",
    "        # TODO: Initialize the Query_Agent agent\n",
    "        self.index = pinecone_index\n",
    "        self.client = openai_client\n",
    "        self.embeddings = embeddings\n",
    "        self.namespace = \"ns2500\"\n",
    "\n",
    "        self.system_prompt = (\n",
    "            \"You are a domain relevance classifier.\\n\"\n",
    "            \"Given a user query and retrieved documents, determine whether \"\n",
    "            \"the documents are relevant to answering the query.\\n\\n\"\n",
    "            \"Respond ONLY with:\\n\"\n",
    "            \"Yes - if the documents are relevant\\n\"\n",
    "            \"No - if the documents are not relevant\\n\"\n",
    "            \"Do not provide explanations.\"\n",
    "        )\n",
    "\n",
    "    def query_vector_store(self, query, k=5):\n",
    "        # TODO: Query the Pinecone vector store\n",
    "        vectorstore = PineconeVectorStore(\n",
    "            index=self.index,\n",
    "            embedding=self.embeddings,\n",
    "            namespace=self.namespace\n",
    "        )\n",
    "\n",
    "        results = vectorstore.similarity_search(query, k=k)\n",
    "        return results\n",
    "\n",
    "    def set_prompt(self, prompt):\n",
    "        # TODO: Set the prompt for the Query_Agent agent\n",
    "        self.system_prompt = prompt\n",
    "\n",
    "    def extract_action(self, response, query = None):\n",
    "        # TODO: Extract the action from the response\n",
    "        decision = response.strip().lower()\n",
    "        if \"yes\" in decision:\n",
    "            return True\n",
    "        elif \"no\" in decision:\n",
    "            return False\n",
    "        else:\n",
    "            # fallback safety\n",
    "            return False\n",
    "    \n",
    "    def check_relevance(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Full pipeline:\n",
    "        - Retrieve documents\n",
    "        - Check relevance\n",
    "        - Return structured result\n",
    "        \"\"\"\n",
    "        documents = self.query_vector_store(query, k=k)\n",
    "\n",
    "        if not documents:\n",
    "            return {\n",
    "                \"is_relevant\": False,\n",
    "                \"documents\": [],\n",
    "                \"raw_matches\": []\n",
    "            }\n",
    "        \n",
    "        # Step 2: Extract text from LangChain Document objects\n",
    "        docs_text_list = [doc.page_content for doc in documents]\n",
    "        docs_text = \"\\n\\n\".join(docs_text_list)\n",
    "\n",
    "        # Step 3: Build relevance checking prompt\n",
    "        user_prompt = (\n",
    "            f\"User Query:\\n{query}\\n\\n\"\n",
    "            f\"Retrieved Documents:\\n{docs_text}\\n\\n\"\n",
    "            \"Are these documents relevant to answering the user's query?\"\n",
    "        )\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        response_text = completion.choices[0].message.content\n",
    "        is_relevant = self.extract_action(response_text, query=query)\n",
    "\n",
    "        return {\n",
    "            \"is_relevant\": is_relevant,\n",
    "            \"documents\": docs_text_list,   # plain text list\n",
    "            \"raw_matches\": documents       # original Document objects\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Agent Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=open_ai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: machine-learning-textbook\n",
      "Index stats: {'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'ns1000': {'vector_count': 662},\n",
      "                'ns2500': {'vector_count': 662},\n",
      "                'ns500': {'vector_count': 1261}},\n",
      " 'total_vector_count': 2585,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# init Pinecone client\n",
    "PINECONE_INDEX_NAME = \"machine-learning-textbook\"\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "existing = [x[\"name\"] for x in pc.list_indexes()]\n",
    "if PINECONE_INDEX_NAME not in existing:\n",
    "    pc.create_index(\n",
    "        name=PINECONE_INDEX_NAME,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    # wait until ready\n",
    "    while not pc.describe_index(PINECONE_INDEX_NAME).status[\"ready\"]:\n",
    "        time.sleep(2)\n",
    "\n",
    "pinecone_index = pc.Index(PINECONE_INDEX_NAME)\n",
    "print(\"Connected to:\", PINECONE_INDEX_NAME)\n",
    "# 3.4 Printout the index stats\n",
    "print(\"Index stats:\", pinecone_index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init query agent\n",
    "query_agent = Query_Agent(\n",
    "    pinecone_index=pinecone_index,\n",
    "    openai_client=open_ai_client,\n",
    "    embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Query: What is supervised learning?\n",
      "Is Relevant: False\n",
      "Number of Documents Retrieved: 5\n",
      "\n",
      "Sample Retrieved Document:\n",
      "context to choose among them – Karen SpärckJones74 a course in machine learning truefalse questions and only one of them is true it is unlikely you will study very long Really the problem is not with the data but rather with the way that you have deﬁned the learning problem That is to say what you c\n",
      "============================================================\n",
      "Query: Explain neural networks.\n",
      "Is Relevant: True\n",
      "Number of Documents Retrieved: 5\n",
      "\n",
      "Sample Retrieved Document:\n",
      "llection of networks each with a different random136 a course in machine learning initialization you can often obtain better solutions that with just one initialization In other words you can train ten networks with different random seeds and then pick the one that does best on held out data Figure \n",
      "============================================================\n",
      "Query: Who won the NBA championship in 2020?\n",
      "Is Relevant: False\n",
      "Number of Documents Retrieved: 5\n",
      "\n",
      "Sample Retrieved Document:\n",
      "playing only allows two to compete at a time You want to set up a way of pairing the teams and having them compete so that you can ﬁgure out which team is best In learning the teams are now the classes and you’re trying to ﬁgure out which class is best1 1 The sporting analogy breaks down a bit for O\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is supervised learning?\",\n",
    "    \"Explain neural networks.\",\n",
    "    \"Who won the NBA championship in 2020?\",  # irrelevant example\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    result = query_agent.check_relevance(query, k=5)\n",
    "\n",
    "    print(\"Is Relevant:\", result[\"is_relevant\"])\n",
    "    print(\"Number of Documents Retrieved:\", len(result[\"documents\"]))\n",
    "\n",
    "    if result[\"documents\"]:\n",
    "        print(\"\\nSample Retrieved Document:\")\n",
    "        print(result[\"documents\"][0][:300])  # print first 300 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answering_Agent:\n",
    "    def __init__(self, openai_client) -> None:\n",
    "        # TODO: Initialize the Answering_Agent\n",
    "        self.client = openai_client\n",
    "        self.system_prompt = (\n",
    "            \"You are a helpful AI assistant.\\n\"\n",
    "            \"Answer the user's question using ONLY the provided documents.\\n\\n\"\n",
    "            \"Rules:\\n\"\n",
    "            \"- Use only the provided documents as knowledge.\\n\"\n",
    "            \"- If the answer is not in the documents, say:\\n\"\n",
    "            \"  'I don't have enough information in the provided documents.'\\n\"\n",
    "            \"- Be clear and concise.\\n\"\n",
    "        )\n",
    "\n",
    "    def generate_response(self, query, docs, conv_history, k=5):\n",
    "        # TODO: Generate a response to the user's query\n",
    "        if not docs:\n",
    "            return \"I don't have enough information in the provided documents.\"\n",
    "\n",
    "        # Combine top-k documents\n",
    "        selected_docs = docs[:k]\n",
    "        docs_text = \"\\n\\n\".join(selected_docs)\n",
    "\n",
    "        # Format conversation history\n",
    "        history_text = \"\"\n",
    "        for i, msg in enumerate(conv_history):\n",
    "            history_text += f\"Turn {i+1}: {msg}\\n\"\n",
    "\n",
    "        user_prompt = (\n",
    "            f\"Conversation History:\\n{history_text}\\n\\n\"\n",
    "            f\"User Question:\\n{query}\\n\\n\"\n",
    "            f\"Relevant Documents:\\n{docs_text}\\n\\n\"\n",
    "            \"Answer the question using the provided documents.\"\n",
    "        )\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "        response = completion.choices[0].message.content.strip()\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering Agent Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised learning is not explicitly defined in the provided documents. However, based on the context, it involves learning from labeled data where the goal is to determine the best class or output based on input features. The documents mention the process of making predictions and the importance of training data, which suggests that supervised learning uses labeled examples to train models to predict outcomes for new, unseen data.\n"
     ]
    }
   ],
   "source": [
    "answering_agent = Answering_Agent(open_ai_client)\n",
    "\n",
    "\n",
    "response = answering_agent.generate_response(\n",
    "    query=\"What is supervised learning?\",\n",
    "    docs=result[\"documents\"],\n",
    "    conv_history=[]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Documents Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relevant_Documents_Agent:\n",
    "    def __init__(self, openai_client) -> None:\n",
    "        # TODO: Initialize the Relevant_Documents_Agent\n",
    "        self.client = openai_client\n",
    "        self.system_prompt = (\n",
    "            \"You are a relevance checking agent.\\n\"\n",
    "            \"Your job is to determine whether the retrieved documents \"\n",
    "            \"are relevant to answering the user's query.\\n\\n\"\n",
    "            \"Respond ONLY with:\\n\"\n",
    "            \"Yes - if the documents are relevant\\n\"\n",
    "            \"No - if they are not relevant\\n\"\n",
    "            \"Do not provide explanation.\"\n",
    "        )\n",
    "\n",
    "    def get_relevance(self, conversation) -> str:\n",
    "        # TODO: Get if the returned documents are relevant\n",
    "        \"\"\"\n",
    "        conversation should contain:\n",
    "        {\n",
    "            'query': str,\n",
    "            'documents': list[str]\n",
    "        }\n",
    "        \"\"\"\n",
    "        query = conversation.get(\"query\", \"\")\n",
    "        documents = conversation.get(\"documents\", [])\n",
    "\n",
    "        if not documents:\n",
    "            return \"No\"\n",
    "\n",
    "        docs_text = \"\\n\\n\".join(documents)\n",
    "\n",
    "        user_prompt = (\n",
    "            f\"User Query:\\n{query}\\n\\n\"\n",
    "            f\"Retrieved Documents:\\n{docs_text}\\n\\n\"\n",
    "            \"Are these documents relevant to the user's query?\"\n",
    "        )\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        response = completion.choices[0].message.content.strip()\n",
    "\n",
    "        # Enforce strict Yes/No output\n",
    "        if \"yes\" in response.lower():\n",
    "            return \"Yes\"\n",
    "        elif \"no\" in response.lower():\n",
    "            return \"No\"\n",
    "        else:\n",
    "            return \"No\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Documents Agent Uage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_agent = Relevant_Documents_Agent(open_ai_client)\n",
    "conversation = {\n",
    "    \"query\": \"What is overfitting?\",\n",
    "    \"documents\": [\n",
    "        \"Overfitting occurs when a model learns noise instead of signal.\",\n",
    "        \"Regularization helps prevent overfitting.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(relevant_agent.get_relevance(conversation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head_Agent:\n",
    "    def __init__(self, openai_key, pinecone_key, pinecone_index_name) -> None:\n",
    "        # TODO: Initialize the Head_Agent\n",
    "        self.openai_client = OpenAI(api_key=openai_key)\n",
    "\n",
    "        self.pc = Pinecone(api_key=pinecone_key)\n",
    "        self.index = self.pc.Index(pinecone_index_name)\n",
    "\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            openai_api_key=openai_key\n",
    "        )\n",
    "\n",
    "        self.conversation_history = []\n",
    "\n",
    "        # Setup sub-agents\n",
    "        self.setup_sub_agents()\n",
    "\n",
    "    def setup_sub_agents(self):\n",
    "        # TODO: Setup the sub-agents\n",
    "        self.obnoxious_agent = Obnoxious_Agent(self.openai_client)\n",
    "        self.context_rewriter = Context_Rewriter_Agent(self.openai_client)\n",
    "        self.query_agent = Query_Agent(\n",
    "            pinecone_index=self.index,\n",
    "            openai_client=self.openai_client,\n",
    "            embeddings=self.embeddings\n",
    "        )\n",
    "        self.relevant_docs_agent = Relevant_Documents_Agent(self.openai_client)\n",
    "        self.answering_agent = Answering_Agent(self.openai_client)\n",
    "\n",
    "    def main_loop(self):\n",
    "        # TODO: Run the main loop for the chatbot\n",
    "        print(\"Multi-Agent Chatbot Started (type 'exit' to quit)\\n\")\n",
    "        while True:\n",
    "            user_input = input(\"User: \")\n",
    "            if user_input.lower() == \"exit\":\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # 1. Rewrite context (multi-turn support)\n",
    "            rewritten_query = self.context_rewriter.rephrase(\n",
    "                self.conversation_history,\n",
    "                user_input\n",
    "            )\n",
    "\n",
    "            # 2. Check obnoxious content\n",
    "            if self.obnoxious_agent.check_query(rewritten_query):\n",
    "                response = \"Your query is inappropriate. Please ask something else.\"\n",
    "                print(\"Bot:\", response)\n",
    "                continue\n",
    "            \n",
    "            # 3. Retrieve documents\n",
    "            retrieved_docs = self.query_agent.query_vector_store(rewritten_query)\n",
    "            docs_text = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "            # 4. Check document relevance\n",
    "            relevance = self.relevant_docs_agent.get_relevance({\n",
    "                \"query\": rewritten_query,\n",
    "                \"documents\": docs_text\n",
    "            })\n",
    "\n",
    "            if relevance == \"No\":\n",
    "                response = \"Your question is outside the scope of this knowledge base.\"\n",
    "                print(\"Bot:\", response)\n",
    "                continue\n",
    "\n",
    "            # 5. Generate final answer\n",
    "            response = self.answering_agent.generate_response(\n",
    "                query=rewritten_query,\n",
    "                docs=docs_text,\n",
    "                conv_history=self.conversation_history\n",
    "            )\n",
    "\n",
    "            # 6. Update conversation history\n",
    "            self.conversation_history.append(f\"User: {user_input}\")\n",
    "            self.conversation_history.append(f\"Bot: {response}\")\n",
    "            print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Agent Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_head_agent():\n",
    "    chatbot = Head_Agent(\n",
    "        openai_key=open_ai_api_key,\n",
    "        pinecone_key=pinecone_api_key,\n",
    "        pinecone_index_name=PINECONE_INDEX_NAME\n",
    "    )\n",
    "\n",
    "    test_queries = [\n",
    "        \"What is overfitting?\",\n",
    "        \"Who won the NBA finals?\",\n",
    "        \"You are useless.\"\n",
    "    ]\n",
    "\n",
    "    for q in test_queries:\n",
    "        print(\"=\"*60)\n",
    "        print(\"User:\", q)\n",
    "\n",
    "        rewritten = chatbot.context_rewriter.rephrase(\n",
    "            chatbot.conversation_history, q\n",
    "        )\n",
    "\n",
    "        if chatbot.obnoxious_agent.check_query(rewritten):\n",
    "            print(\"Bot: Inappropriate query.\")\n",
    "            continue\n",
    "        \n",
    "        docs = chatbot.query_agent.query_vector_store(rewritten)\n",
    "        docs_text = [doc.page_content for doc in docs]\n",
    "\n",
    "        relevance = chatbot.relevant_docs_agent.get_relevance({\n",
    "            \"query\": rewritten,\n",
    "            \"documents\": docs_text\n",
    "        })\n",
    "\n",
    "        if relevance == \"No\":\n",
    "            print(\"Bot: Outside knowledge base.\")\n",
    "            continue\n",
    "\n",
    "        response = chatbot.answering_agent.generate_response(\n",
    "            rewritten,\n",
    "            docs_text,\n",
    "            chatbot.conversation_history\n",
    "        )\n",
    "\n",
    "        print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "User: What is overfitting?\n",
      "Bot: In the context of machine learning, \"overfitting\" occurs when a model pays too much attention to the idiosyncrasies or noise in the training data and is not able to generalize well to new, unseen data. It often means that the model is fitting noise rather than the underlying pattern it is supposed to learn.\n",
      "============================================================\n",
      "User: Who won the NBA finals?\n",
      "Bot: Outside knowledge base.\n",
      "============================================================\n",
      "User: You are useless.\n",
      "Bot: Outside knowledge base.\n"
     ]
    }
   ],
   "source": [
    "test_head_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project Part-4: Evaluating a Multi-Agent Chatbot (50 points)\n",
    "\n",
    "## Goal\n",
    "This part focuses on the \"LLM-as-a-Judge\" paradigm, where you will design a comprehensive benchmark to evaluate your multi-agent system's capabilities.\n",
    "\n",
    "## Action Items\n",
    "\n",
    "### 1. Develop the Test Dataset\n",
    "Create a dataset of **50 prompt/response pairs** to test your bot. While you can curate these manually, you are encouraged to use a synthetic generation strategy (e.g., prompting GPT-4 to generate diverse test cases). The dataset must include:\n",
    "\n",
    "* **Basic Test Cases:**\n",
    "    * **Obnoxious Queries:** 10 prompts designed to trigger the `Obnoxious_Agent` where we want refusal (e.g., \"Explain machine learning, idiot\").\n",
    "    * **Irrelevant Queries:** 10 prompts completely unrelated to your indexed Pinecone data where we want refusal (e.g., \"Who won the super bowl in 2026?\").\n",
    "    * **Relevant Queries:** 10 prompts directly addressed by your indexed documents where we do not want a refusal (e.g., \"Explain logistic regression.\").\n",
    "    * **Greetings/Small Talk:** 5 prompts where we do not want a refusal (e.g., \"Hello\", \"Good morning\").\n",
    "* **Advanced Test Cases:**\n",
    "    * **Hybrid Prompts:** 8 prompts containing a mixture of relevant and irrelevant/obnoxious content (e.g., \"Tell me about Machine Learning and then tell me the capital of France.\"). The bot must isolate and respond **only** to the relevant part.\n",
    "    * **Multi-turn Conversations:** 7 scenarios involving 2-3 turns each, specifically testing context retention of **previous relevant user inputs and bot outputs**. For example, if a user says something obnoxious but then later asks a relevant question, the agent should still respond.\n",
    "\n",
    "### 2. Implement the \"LLM-as-a-Judge\" Agent\n",
    "Create a new evaluation script or agent that acts as a judge. This agent will take the `User Input`, the `Chatbot Response`, and the `Chatbot Agent Path` (which agent generated the final answer) to score the performance. For now, we just want to make sure that the agent behaves correctly and we do not need to evaluate whether or not the models final response is factually correct. \n",
    "\n",
    "* **Judge Capability: Binary Classification:** \n",
    "    * The judge must accurately classify if the chatbot **Responded** (generated an answer) or **Refused** (blocked for safety/relevancy). It should produce a score of **1** when the chatbot exhibits the desired response and **0** otherwise.\n",
    "    * For hybrid prompts, a score of **1** should be produced only when the model refuses or ignores the irrelevant component and answers the relevent component. If either of these criteria is violated, produce a score of **0**.\n",
    "    * For multi-turn conversations, you should only evaluate the last response. For example, if the history contains the following: 1 query/response about logistic regression  and the follow up question is the following: \"Tell me more about it\", the response should not \n",
    "\n",
    "\n",
    "### 3. Compute Aggregated Metrics\n",
    "Run your test prompts through the chatbot, collect the response from the judge, and compute the overall performance by summing up the individual scores.\n",
    "\n",
    "\n",
    "## Deliverables\n",
    "1.  The Python scripts containing the test dataset generation/loading logic, the LLM Judge prompt engineering, and the execution loop.\n",
    "2. **`test_set.json`**: A JSON file that contains the actual test prompts that you used.\n",
    "3. Documentation that briefly describes your data generation approach, and reports the final metric. You should describe some weaknesses of your agent.\n",
    "\n",
    "## Evaluation Criteria\n",
    "1. Completness: Does the test set contain all the types of prompts? (25 points)\n",
    "2. Soundness: Do the provided prompts make sense? Are they realistic? Are they diverse? (10 points)\n",
    "3. Documentation: Is the process well documented with descriptions on how the data was generated, failure modes of the agent, and the final performance? (15 points) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class TestDatasetGenerator:\n",
    "    \"\"\"\n",
    "    Responsible for generating and managing the test dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, openai_client) -> None:\n",
    "        self.client = openai_client\n",
    "        self.dataset = {\n",
    "            \"obnoxious\": [],\n",
    "            \"irrelevant\": [],\n",
    "            \"relevant\": [],\n",
    "            \"small_talk\": [],\n",
    "            \"hybrid\": [],\n",
    "            \"multi_turn\": []\n",
    "        }\n",
    "\n",
    "    def generate_synthetic_prompts(self, category: str, count: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Uses an LLM to generate synthetic test cases for a specific category.\n",
    "        \"\"\"\n",
    "        # TODO: Construct a prompt to generate 'count' examples for 'category'\n",
    "        # TODO: Parse the LLM response into a list of strings or dictionaries\n",
    "        pass\n",
    "\n",
    "    def build_full_dataset(self):\n",
    "        \"\"\"\n",
    "        Orchestrates the generation of all required test cases.\n",
    "        \"\"\"\n",
    "        # TODO: Call generate_synthetic_prompts for each category with the required counts:\n",
    "        pass\n",
    "\n",
    "    def save_dataset(self, filepath: str = \"test_set.json\"):\n",
    "        # TODO: Save self.dataset to a JSON file\n",
    "        pass\n",
    "\n",
    "    def load_dataset(self, filepath: str = \"test_set.json\"):\n",
    "        # TODO: Load dataset from JSON file\n",
    "        pass\n",
    "\n",
    "\n",
    "class LLM_Judge:\n",
    "    \"\"\"\n",
    "    The 'LLM-as-a-Judge' that evaluates the chatbot's performance.\n",
    "    \"\"\"\n",
    "    def __init__(self, openai_client) -> None:\n",
    "        self.client = openai_client\n",
    "\n",
    "    def construct_judge_prompt(self, user_input, bot_response, category):\n",
    "        \"\"\"\n",
    "        Constructs the prompt for the Judge LLM.\n",
    "        \"\"\"\n",
    "        # TODO: Create a prompt that includes:\n",
    "        # 1. The User Input\n",
    "        # 2. The Chatbot's Response\n",
    "        # 3. The specific criteria for the category (e.g., Hybrid must answer relevant part only)\n",
    "        pass\n",
    "\n",
    "    def evaluate_interaction(self, user_input, bot_response, agent_used, category) -> int:\n",
    "        \"\"\"\n",
    "        Sends the interaction to the Judge LLM and parses the binary score (0 or 1).\n",
    "        \"\"\"\n",
    "        # TODO: Call OpenAI API with the judge prompt\n",
    "        # TODO: Parse the output to return 1 (Success) or 0 (Failure)\n",
    "        pass\n",
    "\n",
    "\n",
    "class EvaluationPipeline:\n",
    "    \"\"\"\n",
    "    Runs the chatbot against the test dataset and aggregates scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, head_agent, judge: LLM_Judge) -> None:\n",
    "        self.chatbot = head_agent # This is your Head_Agent from Part-3\n",
    "        self.judge = judge\n",
    "        self.results = {}\n",
    "\n",
    "    def run_single_turn_test(self, category: str, test_cases: List[str]):\n",
    "        \"\"\"\n",
    "        Runs tests for single-turn categories (Obnoxious, Irrelevant, etc.)\n",
    "        \"\"\"\n",
    "        # TODO: Iterate through test_cases\n",
    "        # TODO: Send query to self.chatbot\n",
    "        # TODO: Capture response and the internal agent path used\n",
    "        # TODO: Pass data to self.judge.evaluate_interaction\n",
    "        # TODO: Store results\n",
    "        pass\n",
    "\n",
    "    def run_multi_turn_test(self, test_cases: List[List[str]]):\n",
    "        \"\"\"\n",
    "        Runs tests for multi-turn conversations.\n",
    "        \"\"\"\n",
    "        # TODO: Iterate through conversation flows\n",
    "        # TODO: Maintain context/history for the chatbot\n",
    "        # TODO: Judge the final response or the flow consistency\n",
    "        pass\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"\n",
    "        Aggregates the scores and prints the final report.\n",
    "        \"\"\"\n",
    "        # TODO: Sum scores per category\n",
    "        # TODO: Calculate overall accuracy\n",
    "        pass\n",
    "\n",
    "# Example Usage Block\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Setup Clients\n",
    "    # client = OpenAI(...)\n",
    "    \n",
    "    # 2. Generate Data\n",
    "    # generator = TestDatasetGenerator(client)\n",
    "    # generator.build_full_dataset()\n",
    "    # generator.save_dataset()\n",
    "\n",
    "    # 3. Initialize System\n",
    "    # head_agent = Head_Agent(...) # From Part 3\n",
    "    # judge = LLM_Judge(client)\n",
    "    # pipeline = EvaluationPipeline(head_agent, judge)\n",
    "\n",
    "    # 4. Run Evaluation\n",
    "    # data = generator.load_dataset()\n",
    "    # pipeline.run_single_turn_test(\"obnoxious\", data[\"obnoxious\"])\n",
    "    # ... (run other categories)\n",
    "    # pipeline.calculate_metrics()\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_proj2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
